{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f2a12c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast,XLMRobertaTokenizer\n",
    "import string\n",
    "import torch.nn as nn\n",
    "from transformers import BertPreTrainedModel, BertModel, BertTokenizerFast\n",
    "from transformers import RobertaPreTrainedModel,XLMRobertaConfig, XLMRobertaTokenizer, XLMRobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47d5e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok2 = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "tok = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "#bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c90e4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTmodel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        #embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.bert(text)[1]\n",
    "        return embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02fdb8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3807035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForPreTraining\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3a6e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Customer reviews indicate that many modern mobile devices are often unnecessarily * .\"\n",
    "candidate = [\"complication\", \"complicates\", \"complicate\", \"complicated\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37c56e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(text)\n",
    "# -> ['customer', 'reviews', 'indicate', 'that', 'many', 'modern', 'mobile', 'devices', 'are', 'often', 'un', '##ne', '##ces', '##sari', '##ly', '*', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10fa55c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_index = tokens.index(\"*\")  # 空欄部分のトークンのインデックスを取得\n",
    "tokens[masked_index] = \"[MASK]\"\n",
    "tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "# -> ['[CLS]', 'customer', 'reviews', 'indicate', 'that', 'many', 'modern', 'mobile', 'devices', 'are', 'often', 'un', '##ne', '##ces', '##sari', '##ly', '[MASK]', '.', '[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e0931819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 101590, 10165, 24860, 35802, 10189, 11299, 13456, 24662, 38120, 10301, 12899, 10119, 54881, 18621, 103866, 103, 119, 102]\n",
      "last_hidden_state pooler_output\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "ids = torch.tensor(ids).reshape(1,-1)  # バッチサイズ1の形に整形\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs1, outputs2 = model(ids)\n",
    "    print(outputs1, outputs2)\n",
    "predictions = outputs1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ae0aa8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1637/4136704032.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmasked_index\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredicted_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_indexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# -> ['expensive', 'small', 'priced', 'used', ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "_, predicted_indexes = torch.topk(predictions[masked_index+1], k=1000)\n",
    "predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_indexes.tolist())\n",
    "# -> ['expensive', 'small', 'priced', 'used', ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b10e7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae378d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038349ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "124c7552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   101,   5949,   1947,    133, 107425,    135,   1911,  10083,    102,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0]]) tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1637/738156466.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 return_tensors='pt', max_length=30)['attention_mask']\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "batch_text = ['私は<mask>か？']\n",
    "ids = tok2(batch_text,padding='max_length', truncation=True,\n",
    "                return_tensors='pt', max_length=30)['input_ids']\n",
    "mask = tok2(batch_text,padding='max_length', truncation=True,\n",
    "                return_tensors='pt', max_length=30)['attention_mask']\n",
    "print(ids,mask)\n",
    "bert(ids,mask).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64da9a2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[250001,  65579, 250001,      6,   1894,     32,      2,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1]]) tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]])\n",
      "tensor([[  101,  5949,  1947,   103,  1911, 10083,   102,   103,   103,   103,\n",
      "           103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "           103,   103,   103,   103,   103,   103,   103,   103,   103,   103]]) tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "batch_text2 = ['私は[MASK]か？']\n",
    "batch_text = ['私は<mask>か？']\n",
    "ids = tok(batch_text,padding='max_length', truncation=True,\n",
    "                return_tensors='pt', max_length=30)['input_ids']\n",
    "\n",
    "ids2 = tok2(batch_text2,padding='max_length', truncation=True,\n",
    "                return_tensors='pt', max_length=30)['input_ids']\n",
    "\n",
    "ids[ids == 0] = tok.mask_token_id\n",
    "ids2[ids2 == 0] = tok2.mask_token_id\n",
    "\n",
    "mask = tok(batch_text,padding='max_length', truncation=True,\n",
    "                return_tensors='pt', max_length=30)['attention_mask']\n",
    "\n",
    "mask2 = tok2(batch_text2,padding='max_length', truncation=True,\n",
    "                return_tensors='pt', max_length=30)['attention_mask']\n",
    "print(ids,mask)\n",
    "print(ids2,mask2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd5d4baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(edgeitems=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cb5bffe",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1637/861423317.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/colbert-v0.2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/colbert-v0.2/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m         )\n\u001b[1;32m    996\u001b[0m         encoder_outputs = self.encoder(\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/colbert-v0.2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/colbert-v0.2/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/colbert-v0.2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/colbert-v0.2/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/colbert-v0.2/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2181\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2183\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "emb = bert(ids,mask)\n",
    "emb = emb.last_hidden_state[2]\n",
    "emb[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0648ebef",
   "metadata": {},
   "source": [
    "[1, 30, 768]\n",
    "1個の文章を30でpaddingしたmBERT　768次元のベクトルが出力\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5a0e96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n",
      "<mask>\n",
      "<s> [PAD]\n",
      "<pad> [unused1]\n",
      "</s> [unused2]\n",
      "<unk> [unused3]\n",
      ", [unused4]\n",
      ". [unused5]\n",
      "▁ [unused6]\n",
      "s [unused7]\n",
      "▁de [unused8]\n",
      "- [unused9]\n",
      "▁a [unused10]\n",
      "a [unused11]\n",
      ": [unused12]\n",
      "e [unused13]\n",
      "i [unused14]\n",
      "▁( [unused15]\n",
      ") [unused16]\n",
      "▁i [unused17]\n",
      "t [unused18]\n",
      "n [unused19]\n",
      "▁- [unused20]\n",
      "▁la [unused21]\n",
      "▁en [unused22]\n",
      "▁in [unused23]\n",
      "▁na [unused24]\n",
      "' [unused25]\n",
      "’ [unused26]\n",
      "... [unused27]\n",
      "▁e [unused28]\n",
      "▁на [unused29]\n",
      "。 [unused30]\n",
      "o [unused31]\n",
      "? [unused32]\n",
      "en [unused33]\n",
      "u [unused34]\n",
      "▁и [unused35]\n",
      "▁o [unused36]\n",
      "、 [unused37]\n",
      "! [unused38]\n",
      "m [unused39]\n",
      "▁se [unused40]\n",
      "▁que [unused41]\n",
      "r [unused42]\n",
      "的 [unused43]\n",
      "▁\" [unused44]\n",
      "▁di [unused45]\n",
      "▁– [unused46]\n",
      "▁to [unused47]\n",
      "▁da [unused48]\n",
      "▁в [unused49]\n",
      "، [unused50]\n",
      "▁un [unused51]\n",
      "▁“ [unused52]\n",
      "y [unused53]\n",
      "▁do [unused54]\n",
      "▁je [unused55]\n",
      "er [unused56]\n",
      "▁sa [unused57]\n",
      "\" [unused58]\n",
      "а [unused59]\n",
      "▁og [unused60]\n",
      "▁за [unused61]\n",
      "▁A [unused62]\n",
      "” [unused63]\n",
      "/ [unused64]\n",
      "▁و [unused65]\n",
      "an [unused66]\n",
      "te [unused67]\n",
      "▁die [unused68]\n",
      "▁да [unused69]\n",
      "▁the [unused70]\n",
      "d [unused71]\n",
      "▁er [unused72]\n",
      "in [unused73]\n",
      "; [unused74]\n",
      "▁u [unused75]\n",
      "na [unused76]\n",
      "▁не [unused77]\n",
      "▁si [unused78]\n",
      "▁ja [unused79]\n",
      "▁za [unused80]\n",
      "▁v [unused81]\n",
      "▁et [unused82]\n",
      "▁is [unused83]\n",
      "▁у [unused84]\n",
      "da [unused85]\n",
      "ne [unused86]\n",
      "▁I [unused87]\n",
      "▁el [unused88]\n",
      "и [unused89]\n",
      "es [unused90]\n",
      "▁s [unused91]\n",
      "k [unused92]\n",
      "ni [unused93]\n",
      "▁« [unused94]\n",
      "▁le [unused95]\n",
      "▁l [unused96]\n",
      "▁z [unused97]\n",
      "▁on [unused98]\n",
      "▁at [unused99]\n",
      "▁for [UNK]\n",
      "▁_ [CLS]\n",
      "ta [SEP]\n",
      "е [MASK]\n",
      "▁d <S>\n",
      "у <T>\n",
      "▁1 !\n",
      "re \"\n",
      "▁ne #\n",
      "▁på $\n"
     ]
    }
   ],
   "source": [
    "print(tok2.convert_ids_to_tokens(103))\n",
    "print(tok.convert_ids_to_tokens(250001))\n",
    "for i in range(110):\n",
    "    print(tok.convert_ids_to_tokens(i),tok2.convert_ids_to_tokens(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b0cf10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250002\n",
      "250003\n"
     ]
    }
   ],
   "source": [
    "print(tok.convert_tokens_to_ids(\"[unused1]\"))\n",
    "print(tok.convert_tokens_to_ids(\"[unused2]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e8d0f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "tok.add_tokens(['[unused1]'])\n",
    "tok.add_tokens(['[unused2]'])\n",
    "tok.add_tokens(['[unused_zho]'])\n",
    "tok.add_tokens(['[unused_fas]'])\n",
    "tok.add_tokens(['[unused_rus]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efd9c1db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     0,      6,      5,      6,  92688,    342,     37,   1065,  40639,\n",
      "            605,    268,  13218,  68907,  11804,     32,      2,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1],\n",
      "        [     0,      6,      5,      6,  92688,    342,     37,   1065, 116908,\n",
      "            268,  13218,  68907,  11804,     32,      2,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1],\n",
      "        [     0,      6,      5,   2583,    621,     10,   7515,      2,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "Qids = tok(batch_text)['input_ids']\n",
    "batch_text = ['. ' + x for x in batch_text]\n",
    "obj = tok(batch_text, padding='max_length', truncation=True,\n",
    "                return_tensors='pt', max_length=30)\n",
    "Qids,Qmask = obj['input_ids'], obj['attention_mask']\n",
    "#Qids[:, 1] = 250003\n",
    "#ids[:, 2] = 250002\n",
    "#Qids[Qids == 1] = tok.mask_token_id\n",
    "print(obj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6364ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1904, 27843, 11588,  1881,   103,  1956,  7069, 20832, 15752,\n",
      "         10083,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1904, 27843, 11588,  1881,     0,  1956,  7069, 20832, 15752,\n",
      "         10083,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 11065, 10301,   169, 41163,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]])}\n",
      "tensor([[  101,  1904, 27843, 11588,  1881,   103,  1956,  7069, 20832, 15752,\n",
      "         10083,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1904, 27843, 11588,  1881,     0,  1956,  7069, 20832, 15752,\n",
      "         10083,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 11065, 10301,   169, 41163,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "['[CLS]', 'あ', '##れ', '##は', '、', '[MASK]', 'へ', '行', 'くの', '##か', '？', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[CLS]', 'あ', '##れ', '##は', '、', '[PAD]', 'へ', '行', 'くの', '##か', '？', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[CLS]', 'You', 'are', 'a', 'cat', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "batch_text = ['私は[MASK]か？']\n",
    "text = tok2(batch_text,padding='max_length', truncation=True,\n",
    "                return_tensors='pt', max_length=30)\n",
    "a = a['input_ids']\n",
    "print(a)\n",
    "for ids in a:\n",
    "    print(tok2.convert_ids_to_tokens(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "73607887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method indices of Tensor object at 0x7f6e10141890>\n",
      "torch.return_types.sort(\n",
      "values=tensor([8, 8, 8]),\n",
      "indices=tensor([0, 1, 2]))\n"
     ]
    }
   ],
   "source": [
    "batch_text = [\"I like a cat\", \"I am a student\",\"You are a cat\"]\n",
    "Qids = tok(batch_text)['input_ids']\n",
    "batch_text = ['. ' + x for x in batch_text]\n",
    "obj = tok(batch_text, padding='max_length', truncation=True,\n",
    "                return_tensors='pt', max_length=15)\n",
    "Qids,mask = obj['input_ids'], obj['attention_mask']\n",
    "print(mask.indices)\n",
    "indices = mask.sum(-1).sort()#.indices\n",
    "print(indices)\n",
    "#reverse_indices = indices.sort().indices\n",
    "#print(reverse_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8cf1084",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_text2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_99371/3612574724.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_text2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbatch_text2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'. '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_text2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m obj = tok(batch_text2, padding='max_length', truncation=True,\n\u001b[1;32m      4\u001b[0m                 return_tensors='pt', max_length=15)\n\u001b[1;32m      5\u001b[0m \u001b[0mDids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_text2' is not defined"
     ]
    }
   ],
   "source": [
    "Dids = tok(batch_text2)['input_ids']\n",
    "batch_text2 = ['. ' + x for x in batch_text2]\n",
    "obj = tok(batch_text2, padding='max_length', truncation=True,\n",
    "                return_tensors='pt', max_length=15)\n",
    "Dids,Dmask = obj['input_ids'], obj['attention_mask']\n",
    "Dids[:, 1] = 250002\n",
    "#ids[:, 2] = 250002\n",
    "Dids[Dids == 1] = tok.mask_token_id\n",
    "print(ids,obj['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "badff418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     0, 250002,      5,     87,   1884,     10,   7515,      2, 250001,\n",
      "         250001, 250001, 250001, 250001, 250001, 250001],\n",
      "        [     0, 250002,      5,     87,    444,     10,   9836,      2, 250001,\n",
      "         250001, 250001, 250001, 250001, 250001, 250001],\n",
      "        [     0, 250002,      5,   2583,    621,     10,   7515,      2, 250001,\n",
      "         250001, 250001, 250001, 250001, 250001, 250001]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 3, -1]' is invalid for input of size 45",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12648/2983231077.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mDids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[2, 3, -1]' is invalid for input of size 45"
     ]
    }
   ],
   "source": [
    "N = len(batch_text)\n",
    "print(Dids)\n",
    "Dids, Dmask = Dids.view(2, N, -1), Dmask.view(2, N, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1431badb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250001\n",
      "<mask>\n",
      "250001\n",
      "250004\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "51a04a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [PAD]\n",
      "<pad> [unused1]\n",
      "</s> [unused2]\n",
      "<unk> [unused3]\n",
      ", [unused4]\n",
      ". [unused5]\n",
      "▁ [unused6]\n",
      "s [unused7]\n",
      "▁de [unused8]\n",
      "- [unused9]\n",
      "▁a [unused10]\n",
      "a [unused11]\n",
      ": [unused12]\n",
      "e [unused13]\n",
      "i [unused14]\n",
      "▁( [unused15]\n",
      ") [unused16]\n",
      "▁i [unused17]\n",
      "t [unused18]\n",
      "n [unused19]\n",
      "▁- [unused20]\n",
      "▁la [unused21]\n",
      "▁en [unused22]\n",
      "▁in [unused23]\n",
      "▁na [unused24]\n",
      "' [unused25]\n",
      "’ [unused26]\n",
      "... [unused27]\n",
      "▁e [unused28]\n",
      "▁на [unused29]\n",
      "。 [unused30]\n",
      "o [unused31]\n",
      "? [unused32]\n",
      "en [unused33]\n",
      "u [unused34]\n",
      "▁и [unused35]\n",
      "▁o [unused36]\n",
      "、 [unused37]\n",
      "! [unused38]\n",
      "m [unused39]\n",
      "▁se [unused40]\n",
      "▁que [unused41]\n",
      "r [unused42]\n",
      "的 [unused43]\n",
      "▁\" [unused44]\n",
      "▁di [unused45]\n",
      "▁– [unused46]\n",
      "▁to [unused47]\n",
      "▁da [unused48]\n",
      "▁в [unused49]\n",
      "، [unused50]\n",
      "▁un [unused51]\n",
      "▁“ [unused52]\n",
      "y [unused53]\n",
      "▁do [unused54]\n",
      "▁je [unused55]\n",
      "er [unused56]\n",
      "▁sa [unused57]\n",
      "\" [unused58]\n",
      "а [unused59]\n",
      "▁og [unused60]\n",
      "▁за [unused61]\n",
      "▁A [unused62]\n",
      "” [unused63]\n",
      "/ [unused64]\n",
      "▁و [unused65]\n",
      "an [unused66]\n",
      "te [unused67]\n",
      "▁die [unused68]\n",
      "▁да [unused69]\n",
      "▁the [unused70]\n",
      "d [unused71]\n",
      "▁er [unused72]\n",
      "in [unused73]\n",
      "; [unused74]\n",
      "▁u [unused75]\n",
      "na [unused76]\n",
      "▁не [unused77]\n",
      "▁si [unused78]\n",
      "▁ja [unused79]\n",
      "▁za [unused80]\n",
      "▁v [unused81]\n",
      "▁et [unused82]\n",
      "▁is [unused83]\n",
      "▁у [unused84]\n",
      "da [unused85]\n",
      "ne [unused86]\n",
      "▁I [unused87]\n",
      "▁el [unused88]\n",
      "и [unused89]\n",
      "es [unused90]\n",
      "▁s [unused91]\n",
      "k [unused92]\n",
      "ni [unused93]\n",
      "▁« [unused94]\n",
      "▁le [unused95]\n",
      "▁l [unused96]\n",
      "▁z [unused97]\n",
      "▁on [unused98]\n",
      "▁at [unused99]\n",
      "▁for [UNK]\n",
      "▁_ [CLS]\n",
      "ta [SEP]\n",
      "е [MASK]\n",
      "▁d <S>\n",
      "у <T>\n",
      "▁1 !\n",
      "re \"\n",
      "▁ne #\n",
      "▁på $\n"
     ]
    }
   ],
   "source": [
    "for i in range(110):\n",
    "    print(tok.convert_ids_to_tokens(i),tok2.convert_ids_to_tokens(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "241d6c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_text = ['.' + x for x in batch_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "484d8d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    6,    5,  568, 1884,   10, 7515,    2,    1,    1,    1,    1,\n",
      "            1,    1,    1],\n",
      "        [   0,    6,    5,  568,  444,   10, 9836,    2,    1,    1,    1,    1,\n",
      "            1,    1,    1]])\n",
      "tensor([[     0, 250002,      5,    568,   1884,     10,   7515,      2,      1,\n",
      "              1,      1,      1,      1,      1,      1],\n",
      "        [     0, 250002,      5,    568,    444,     10,   9836,      2,      1,\n",
      "              1,      1,      1,      1,      1,      1]])\n"
     ]
    }
   ],
   "source": [
    "obj = tok(batch_text, padding='max_length', truncation=True,\n",
    "                return_tensors='pt', max_length=15)\n",
    "ids, mask = obj['input_ids'], obj['attention_mask']\n",
    "print(ids)\n",
    "ids[:, 1] = 250002\n",
    "#ids[ids == 0] = tok.mask_token_id\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f29fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'稣'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.convert_ids_to_tokens(250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f75c23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.add_tokens(['[D]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf4855f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80ab52d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [250002], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok(\"[D]\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fa0b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
